{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Markov Decision Processes (MDPs)\n",
    "\n",
    "[source](https://www.youtube.com/watch?v=2GwBez0D20A)\n",
    "\n",
    "This notebook will present information on how to solve small-scale Markov decision process problems using two methods:\n",
    "\n",
    "- **value iteration**,\n",
    "- **policy iteration**.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MDP\n",
    "\n",
    "<div style=\"display: flex; justify-content: space-between;\">\n",
    "\n",
    "<div style=\"flex: 1; padding-right: 20px;\">\n",
    "MDP is defined by:\n",
    "\n",
    "- set of states $S$\n",
    "- set of actions $A$\n",
    "- transition probability $P(s'|s,a)$\n",
    "- reward function $R(s,a,s')$\n",
    "- start state $s_0$\n",
    "- discount factor $\\gamma$\n",
    "- Horizon $H$\n",
    "</div>\n",
    "\n",
    "<div style=\"flex: 1; text-align: center;\">\n",
    "<img src=\"images/mdp.png\" width=\"500\">\n",
    "</div>\n",
    "\n",
    "</div>\n",
    "\n",
    "The goal is to find an optimal policy $\\pi$ that maximizes the expected cumulative reward:\n",
    "\n",
    "$\n",
    "\\max_{\\pi} \\mathbb{E} \\left[ \\sum_{t=0}^{H} \\gamma^t R(s_t, a_t, s_{t+1}) \\right | \\pi]\n",
    "$\n",
    "\n",
    "or\n",
    "\n",
    "$\n",
    "\\pi^* = \\arg\\max_{\\pi} \\mathbb{E}_{\\pi} \\left[ \\sum_{t=0}^{H} \\gamma^t R(s_t, a_t, s_{t+1}) \\right]\n",
    "$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Policy $\\pi$\n",
    "\n",
    "A policy (usually denoted as $\\pi$) is essentially the strategy or decision-making rule that an agent follows to **decide what action to take in any given state**.\n",
    "\n",
    "You can think of it as the agent's _\"behavior policy\"_ or _\"game plan\"_.\n",
    "\n",
    "A policy can be:\n",
    "\n",
    "- **Deterministic** - The policy maps each state to a single action $a = \\pi(s)$.\n",
    "- **Stochastic** - The policy maps each state to a distribution over actions $a \\sim \\pi(a|s)$.\n",
    "\n",
    "### Value Function $V^{\\pi}(s)$\n",
    "\n",
    "The value function $V^{\\pi}(s)$ represents the expected cumulative reward an agent can expect to receive starting from state $s$ and following policy $\\pi$ for the remaining of the horizon $H$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TODO\n",
    "\n",
    "https://www.youtube.com/watch?v=2GwBez0D20A\n",
    "\n",
    "Continue from 19:18\n",
    "\n",
    "Create own implemenation of GridWorld environment using Gymnasium\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
